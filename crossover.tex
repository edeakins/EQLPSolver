\documentclass[runningheads]{llncs}

%\documentclass{article}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[]{algorithm2e}
%\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{cite}
\usepackage{etoolbox}

%\usepackage{amsthm,pifont}

\usepackage[utf8]{inputenc}
%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}{Corollary}[theorem]

\newcommand{\cP}{{\mathcal P}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\cO}{{\mathcal O}}
\newcommand{\cC}{{\mathcal C}}
\newcommand{\cV}{{\mathcal V}}


\begin{document}
%\title{Neighborhood-based Reductions and Cuts for Signed Graphs}
%\author{Christopher Muir$^1$, Tony K. Rodriguez$^{1,2}$, James A. Ostrowski$^1$}
%\date{}
%\maketitle
%\begin{abstract}

%\end{abstract}



Consider a linear program $LP(A,b,c)$ in the following form:
\begin{align} \label{eq:LP}
  \max \ & c^t x \\
  \mbox{s.t. } & Ax \leq b\\
  & x \geq 0,
  \end{align}
\noindent with  $x$ and  $c$ are in $\mathbb{R}^n$, $b$ is in $\mathbb{R}^m$ and
$A \in \mathbb{R}^{m \times n}$.

Let $\cP = \{P_1,\ P_2,\ \ldots,\ P_k\}$ be partition of the decision variables. We define the restricted
linear program, $RLP(A,B,c,\cP)$ to be:

\begin{align}
  \max \ & c^t x \\
  \mbox{s.t. } & Ax \leq b\\
  & x_i = x_i \ \ \forall\ i,j \ \in P_l,\  l= 1,\ \ldots,\ k \label{cons:equal}\\
  & x \geq 0,
  \end{align}

\noindent Obviously can be written in such a way as to aggregate variables that
belong in the same partition. Such an aggregation will yield a formulation with
only $k$ variables and (potentially) fewer constraints. We will refer to the aggregated version as $ARLP(A,b,c,\cP)$.

 It is easier, however, to
discuss the proposed methods using the above formulation as both, as stated,  $LP(A,b,c)$ and
$RLP(A,B,c,\cP)$ exist in the same dimension.

For this work we are primarily focused on partitions $\cP$ such that the optimal
solution to $LP(A,b,c)$ is equal to that of $RLP(A,B,c,\cP)$. We note that
techniques have been developed that attempt find such $\cP$, most notably those
that seek to exploit symmetry and its generalization, equitable partitions. In
such cases, the benefit of  $RLP(A,B,c,\cP)$ is clear, preprocessing will result
in a smaller formulation that will likely be easier to solve. However, the
solutions returned by solving the restricted problem will necessarily be in the
interior of the optimal face (unless $\cP$ consists only of singletons). In many
cases it is desirable to find an optimal vertex to $LP(A,b,c)$. Various
crossover methods have been developed to map an interior solution to an optimal
vertex, but such methods tend to be very computationally taxing. We show here
that mapping a solution from restricted problems generated by symmetry and/or
equitable partitions to optimal vertices are
considerably easier than the more general crossover methods.


\section{Background: Symmetry and Equitable Partitions}

{\em Symmetries and Fractional Symmetries:}

The symmetry group of $LP(A,b,c)$ is defined to
be permutations that map feasible solutions to the LP to feasible solutions with
the same objective value. Let $\cG$ represent the symmetry group. A problem's
symmetry group can be used to define equivalence classes on both the sets of
variables as well as the set of feasible solutions. As permutations act on
vectors, it is most natural to think of how symmetries act on solution vectors.
We say two solutions are equivalent if they have the same objective values and there exists a permutation in $\cG$ that
maps one solution to the other. Computing $\cG$ is not practical, as it requires the knowledge of all feasible solutions. In practice, the {\em formulation group} $\cF$, is used to approximate $\cG$. Note that $\cF \subset \cG$. The formulation group of \eqref{eq:LP} contains the set of
	permutations  $(P_\pi,\ P_\sigma)$ such that:
	\begin{align}  \label{eq:formulation}
	c  P_\pi = c, \\ \notag
	P_\sigma b =b,\\ \notag
	A  P_\pi = P_\sigma A, \\ \notag
	P_\pi \in S^n,\ P_\sigma \in S^m,
	\end{align}

\noindent where $S_\pi$ and $S_\sigma$ are the sets of all permutation matrices of appropriate size. This ensures that integer solutions are mapped to other integer solutions, something that is important in integer programming but not in linear programming. Dropping the integrality restriction gives the set of {\em fractional symmetries}, $\cF$, that contain the set of doubly stochastic matrices that satisfy:
	\begin{align}  \label{eq:fractional}
c  P_\pi = c, \\ \notag
P_\sigma b =b,\\ \notag
A  P_\pi = P_\sigma A, \\ \notag
P_\pi \in D^n,\ P_\sigma \in D^m,
\end{align}
where $D_\pi$ and $D_\sigma$ are the sets of all doubly stochastic matrices of appropriate size. Note that the set of all fractional symmetries can be thought of as a polyhedron (the no longer form a group). 

{\em Orbits and Generalizations:}


We call the set of all solutions equivalent to a
solution $x$ an {\em orbit} of $x$ with respect to $\cG$. We use
$\mbox{Orb}(x,\ \cG)$ to represent the orbit of $x$ with respect to $\cG$. We
overload the $\mbox{Orb}$ notation to act on variables as well as vectors. The
orbit of variable $x_i$ with respect to $\cG$ are those variables that are
equivalent to $x_i$. We have that $j \in \mbox{Orb}(i,\cG)$ if and only if $e_j
\in \mbox{Orb}(e_i,\cG)$. The variable orbits can be used to define a natural
partition of the variables as $i,\ j \in \mbox{Orb}(h,\cG)$ implies that $h, j
\in \mbox{Orb}(i,\cG)$. We let $\cO= \{O_1,\ldots,\ O_k\}$ represent the {\em orbital partition} of the variables. 

While not immediately obvious, equitable partitions are the fractional symmetry analog of orbits. Formally, an equitable partition $(\cV, \cC)$ with respect to $\cF$, which we denote as $EQ(\cP = (\cV,\cC), \cF)$, is a partition of the variables ($\cV = (V_1,\ \ldots,\ V_k)$) and constraints ($\cC = (C_1,\ \ldots,\ C_k)$) that satisfies the following constraints:
\begin{align}
\sum_{p \in C} A_{i,p} - A_{j,p} = 0 & \ \forall i,j \in V \in \cV,\ C \in \cC\\
\sum_{i \in V} A_{i,p} - A_{i,q} = 0 & \ \forall p,q \in C \in \cC, V \in \cV 
\end{align}


We say that partition $\cP^2$ is a {\em refinement} of partition $\cP^1$ if for
all $P_i^2 \in \cP^2$ there exists a $P_j^1 \in \cP^1$ with $P_i^2 \subseteq
P_j^1$. It can be shown that any orbital partition is a refinement of an equitable partition. Conversely, $P^1$ is {\em coarser} than $P_2$. The coarsest equitable partition is easily computed and is often used as the first step in computing the formulation group of an instance. 

In both orbits and equitable partitions, we choose one element from each partition and refer to it as the {\em representative}. For the sake of this paper, we will say that the representative of each partition is the variable/constraint with the smallest index.  

{\em Stabilizers and Generalizations:}

The {\em stabilizer} of a group $\cG$ with respect to an element $v$, $stab(v,\cG)$ is defined to be:

$$stab(v,\cG) := \{ \pi \in \cG\ | \ \pi(v) = v\}.$$ 

\noindent Note that this definition allows for  $v$ to be either a vector or a variable. Using the constraints~\eqref{eq:formulation}, this can be thought of adding the constraint $P_\pi v = v$, or, in the case where $v$ represents a variable, say $x_i$, by fixing $P_\pi(i,i)$ to one. 

Similar to the stabilizer, we wish to {\em isolate} elements of an equitable partition by creating a refinement where that element is in a singleton partition. Formally, we have $iso(v, \cF)$ is define to be:


$$iso(v,\cF) := \{ \pi \in \cF\ | \ \pi(v) = v\}.$$ 
\noindent Again, this can be thought of adding the constraint $P_\pi v = v$, or, in the case where $v$ represents a variable, say $x_i$, by fixing $P_\pi(i,i)$ to one. 



\section{Stuff}

A result from Grohe...
\begin{theorem} \label{thm:set_equal}
	Let $\cF$ be the set of fractional symmetries acting on a linear program and let $\cP = (\cV, \cC)$ be the
	orbital partition with respect to $\cF$. The optimal solution
	value of $LP(A,b,c)$ is equal to the optimal solution value of $RLP(A,b,c, \cP)$
\end{theorem}


\begin{corollary}
	As a result of these fixings, an aggregated LP with the same optimal objective
	value can be written using $|\cV|$ many variables and $|\cC|$ many constraints.
\end{corollary}



In general, if $\cP^2$ is a refinement of $\cP^1$, then the LP solution
to $RLP(A,B,c,\cP_1)$ is no smaller than $RLP(A,B,c,\cP_2)$. Thus, for any
partition that is a refinement of the problem's equitable partition $\cP$,
Theorem~\ref{thm:set_equal} ensures that $RLP(A,b,c,\cP_1)$ has the same
objective value of $LP(A,b,c)$.




{\em Dimensions of RLP polyhedra:}

Note that the polyhedra defined by the feasible region of both $LP(A,b,c)$ and
$RLP(A,b,c,\cP)$ are both in $\mathbb{R}^n$. However, while the feasible
region of $LP(A,b,c)$ can be fully dimensional, that of $RLP(A,b,c,\cP)$ is not
(except for trivial partitions). Indeed, it is easy to see that the set of
equalities defined in~\eqref{cons:equal} have a rank of $n - |\cP|$.

{\em Hierarchy of  optimal restrictions:}

We new define a hierarchy of restrictions that are all guaranteed to share the
same optimal objective value (that of the original LP). We define $iso^0$ to just be $\cF$ and recursively define $iso^k(\cF)$
to be $iso( k , iso^{k-1}(\cF))$. As every $iso^k$ is a set of fractional symmetries, it can be used
to generate the equitable partition $\cP^k$ (with respect to $iso^k$). As $iso^{k+1}$
is a subset of $iso^k$, we have that $\cP^{k+1}$ is a refinement of $\cP^k$.
Thus, $\{dim(RLP(A,b,c,\cP^k)\}_{ k }$ is a nondecreasing
sequence and that $\cP^n$ consists of only singletons.



{\em Crossover}

We use the above facts to design a crossover strategy that will take a vertex
solution from $ARLP(A,b,c,\cP^k\}$ and  lift it to a vertex solution to
$ARLP(A,b,c,\cP^{k+1})$. By iterating through, this will eventually lead to a vertex to $ARLP(A,b,c,\cP^{n}) = LP(A,b,c)$







This will be done iteratively by lifting each
vertex solution of  $RLP(A,b,c,\cO^i\}$  to a vertex solution to
$RLP(A,b,c,\cO^{i+1})$ 



\section{Required Theory}

Things to show(?)
\begin{itemize}
\item Active constraints (bounds) using $\cO^k$ are active in $\cO^{k+1}$: this isn't
  super obvious in the aggregated model, but is in the restricted version(?)
  \item When one constraint becomes active, so are all others in its respective
    orbit, defined by *some* stabilizer. We have to be a bit careful in showing
    this. Crossing over  from $\cO^k$ to $\cO^{k+1}$, the stabilizer we care
    about is $stab^{k+1}$

    \end{itemize}

    Things to consider in the algorithm:
    \begin{itemize}
    \item Three types of pivots, degenerate, those that hit a singleton
      constraint orbit, and those that hit a singleton constraint, and those
      that hit a nontrivial constraint orbit.
      \begin{itemize}
      \item Degenerate: We can drop the corresponding equality constraint and
        move on.
      \item Singleton: We add one linearly independent active constraint
        \item Non trivial: we add possibly more than one linearly independent
          active constraint. To determine this, we only need to compute a
          maximal set of linearly independent constraints for that orbit
          (proof). Can we determine this using the symmetry group instead?
      \end{itemize}
      \end{itemize}
      


{\em Not entirely sure why I put all of this here... Pick and choose...}


{\bf Orbits of Vertices}

Similar to variables, permutations can act on vectors. We can use symmetry to
compute how many equivalent vertices there are in a polyhedron. First, recall
the Orbit-Stabilizer Theorem (OST):

\begin{theorem}
For $x \in \mathbb{R}^n$ and group $\cG$, we have $|\mbox{Orb}(x,\cG)| = \frac{ |\cG|}{|\mbox{Stab}(x,\cG)|}$.
  \end{theorem}

The intuition behind the OST is as follows. Some
permutations in $\cG$ map $x$ to itself. These permutations are those that make
up $\mbox{Stab}(x,\cG)$. By somehow dividing $\cG$ by these permutations we will
be left with permutations that map $x$ to a unique vector.

Using OST, then, for a vertex $x^i$, the set of equivalent vertices contains
$\frac{ |\cG|}{|\mbox{Stab}(x^i,\cG)|}$. Let $B^i$ and $NB^i$ be the set of
basic and nonbasic variables associated with vertex $x^i$. Because $NB^i$ is
enough to uniquely determine $x^i$, we have that $\mbox{Stab}(x^i,\cG) =
\mbox{Stab}(NB^i,\cG)$. Also, as $\mbox{Stab}(S,\cG) =
\mbox{Stab}(\overline{S},\cG)$ for any set $S$, we also have that $\mbox{Stab}(x^i,\cG) =
\mbox{Stab}(B^i,\cG)$. Thus, we can compute the size of the orbit of a given
vertex by just using the basis information (the actual solution doesn't have to
be known). 

In addition the the number of equivalent vertices, symmetry information can be
used to identify relationships between active and inactive constraints. Consider
the following theorem:

\begin{theorem}
For vertex $x^i$, any constraint orbit generated with respect to
$\mbox{Stab}(NB^i,\cG)$ contains either all binding constraints or all inactive constraints. 
\end{theorem}

Note that the above theorem relies on the stabilizer of $x^i$, not the whole
group. Indeed, the size of the orbits with respect to the stabilizer will be no larger than
those with respect to $\cG$. The solution to the aggregated solution can be used
to give insight on the set of active constraints for the corresponding vertices.


\begin{theorem}
For a vertex $x^i$, let $\overline{x}$ be the average of all vertices in
$\mbox{Orb}(x^i, \cG)$. Then, for any constraint orbit $O^C$, either all
constraints in that orbit are binding or none of them are. 
  \end{theorem}

\begin{corollary}
For a vertex $x^i$, let $\overline{x}$ be the average of all vertices in
$\mbox{Orb}(x^i, \cG)$. If a constraint is binding for $\overline{x}$, than the
constraint is binding for all vertices in $\mbox{Orb}(x^i, \cG)$.
  \end{corollary}







\begin{algorithm}[H]
 \KwData{Aggregated solution $x^0$, set of active constraints $C$, group $\cG$}
 \KwResult{A vertex solution to the original LP}
% initialization\;
 \While{$rank(C) < n$}{
  Let $\cG = \mbox{Stab}(C, \cG)$, generate orbital partition of variables
  $\cO = \{O^1,\ \ldots, O^k\}$\;
  Create LP of type~\eqref{} using $\cO$\;

\While{$rank(C) < n$}{
  Select orbit $O^i = \{x_{i_1},\ \ldots,\ x_{i_p}\}$\;
  Compute $\mbox{Stab}( x_{i_1},\cG)$ 
  }

 }
 \caption{How to write algorithms}
\end{algorithm}



\end{document}
